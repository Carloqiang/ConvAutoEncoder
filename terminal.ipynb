{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"terminal.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"KsSUOcUYHYwy","colab_type":"code","colab":{}},"source":["import os\n","os.chdir('/content/drive/My Drive/ConvAutoEncoder')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vnhjk6rj7-Cb","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qGLkj7NZDc5Q","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"outputId":"19527d60-ebc3-4886-f19e-3b11b1b2de4f","executionInfo":{"status":"ok","timestamp":1563797840348,"user_tz":0,"elapsed":31173,"user":{"displayName":"YOUSSEF ZYAM","photoUrl":"","userId":"03615834181023021007"}}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rc9GzqwcIWKm","colab_type":"code","colab":{}},"source":["import glob\n","\n","bl = [x.replace('/content/drive/My Drive/ConvAutoEncoder/data/blurry_images/', '') for x in glob.glob('/content/drive/My Drive/ConvAutoEncoder/data/blurry_images/*.jpg')]\n","nr = [x.replace('/content/drive/My Drive/ConvAutoEncoder/data/normal_images/', '') for x in glob.glob('/content/drive/My Drive/ConvAutoEncoder/data/normal_images/*.jpg')]\n","\n","binn = []\n","ninb = []\n","for b in bl:\n","  if b not in nr : print(b)\n","    \n","for n in nr:\n","  if n not in bl: print(n)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mg0LMvpHRWlC","colab_type":"code","colab":{}},"source":["print(len(bl))\n","print(len(nr))\n","print(len(binn))\n","print(len(ninb))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uRoIKVTdIZyj","colab_type":"code","outputId":"49cb83e6-4ba4-43de-c74a-3600d7f245c8","executionInfo":{"status":"ok","timestamp":1563807116435,"user_tz":0,"elapsed":9055766,"user":{"displayName":"YOUSSEF ZYAM","photoUrl":"","userId":"03615834181023021007"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!python3 main.py --batch_size 32"],"execution_count":11,"outputs":[{"output_type":"stream","text":["======> init weight\n","resuming by loading epoch 001\n","/usr/local/lib/python3.6/dist-packages/torch/serialization.py:454: SourceChangeWarning: source code of class 'Model.ConvAutoencoder' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n","  warnings.warn(msg, SourceChangeWarning)\n","Train Epoch: 2 [0/457 (0%)]\tLoss: 0.010792\n","Train Epoch: 2 [50/457 (5%)]\tLoss: 0.013347\n","Train Epoch: 2 [100/457 (11%)]\tLoss: 0.012940\n","Train Epoch: 2 [150/457 (16%)]\tLoss: 0.010944\n","Train Epoch: 2 [200/457 (22%)]\tLoss: 0.010307\n","Train Epoch: 2 [250/457 (27%)]\tLoss: 0.010465\n","Train Epoch: 2 [300/457 (33%)]\tLoss: 0.010983\n","Train Epoch: 2 [350/457 (38%)]\tLoss: 0.009919\n","Train Epoch: 2 [400/457 (44%)]\tLoss: 0.013522\n","Train Epoch: 2 [450/457 (49%)]\tLoss: 0.010811\n","Train Epoch: 2 [500/457 (55%)]\tLoss: 0.011544\n","Train Epoch: 2 [550/457 (60%)]\tLoss: 0.009246\n","Train Epoch: 2 [600/457 (66%)]\tLoss: 0.011816\n","Train Epoch: 2 [650/457 (71%)]\tLoss: 0.008879\n","Train Epoch: 2 [700/457 (77%)]\tLoss: 0.012594\n","Train Epoch: 2 [750/457 (82%)]\tLoss: 0.009687\n","Train Epoch: 2 [800/457 (88%)]\tLoss: 0.010001\n","Train Epoch: 2 [850/457 (93%)]\tLoss: 0.012136\n","Train Epoch: 2 [900/457 (98%)]\tLoss: 0.012646\n","2019-07-22 14:00:33: epoch =    2 , loss = 0.0108 , time = 5966.44 s\n","\n","Test set: Average loss: 0.0001\n","\n","Train Epoch: 3 [0/457 (0%)]\tLoss: 0.008688\n","Train Epoch: 3 [50/457 (5%)]\tLoss: 0.012699\n","Train Epoch: 3 [100/457 (11%)]\tLoss: 0.013197\n","Train Epoch: 3 [150/457 (16%)]\tLoss: 0.012252\n","Train Epoch: 3 [200/457 (22%)]\tLoss: 0.011989\n","Train Epoch: 3 [250/457 (27%)]\tLoss: 0.008723\n","Train Epoch: 3 [300/457 (33%)]\tLoss: 0.010334\n","Train Epoch: 3 [350/457 (38%)]\tLoss: 0.011724\n","Train Epoch: 3 [400/457 (44%)]\tLoss: 0.011627\n","Train Epoch: 3 [450/457 (49%)]\tLoss: 0.009289\n","Train Epoch: 3 [500/457 (55%)]\tLoss: 0.012485\n","Train Epoch: 3 [550/457 (60%)]\tLoss: 0.011416\n","Train Epoch: 3 [600/457 (66%)]\tLoss: 0.010237\n","Train Epoch: 3 [650/457 (71%)]\tLoss: 0.008597\n","Train Epoch: 3 [700/457 (77%)]\tLoss: 0.011243\n","Train Epoch: 3 [750/457 (82%)]\tLoss: 0.010630\n","Train Epoch: 3 [800/457 (88%)]\tLoss: 0.009382\n","Train Epoch: 3 [850/457 (93%)]\tLoss: 0.010599\n","Train Epoch: 3 [900/457 (98%)]\tLoss: 0.008652\n","2019-07-22 14:29:03: epoch =    3 , loss = 0.0108 , time = 608.43 s\n","\n","Test set: Average loss: 0.0001\n","\n","Train Epoch: 4 [0/457 (0%)]\tLoss: 0.010393\n","Train Epoch: 4 [50/457 (5%)]\tLoss: 0.011572\n","Train Epoch: 4 [100/457 (11%)]\tLoss: 0.010967\n","Train Epoch: 4 [150/457 (16%)]\tLoss: 0.015438\n","Train Epoch: 4 [200/457 (22%)]\tLoss: 0.008678\n","Train Epoch: 4 [250/457 (27%)]\tLoss: 0.009593\n","Train Epoch: 4 [300/457 (33%)]\tLoss: 0.012036\n","Train Epoch: 4 [350/457 (38%)]\tLoss: 0.008867\n","Train Epoch: 4 [400/457 (44%)]\tLoss: 0.010064\n","Train Epoch: 4 [450/457 (49%)]\tLoss: 0.011893\n","Train Epoch: 4 [500/457 (55%)]\tLoss: 0.010537\n","Train Epoch: 4 [550/457 (60%)]\tLoss: 0.013469\n","Train Epoch: 4 [600/457 (66%)]\tLoss: 0.010001\n","Train Epoch: 4 [650/457 (71%)]\tLoss: 0.009442\n","Train Epoch: 4 [700/457 (77%)]\tLoss: 0.009978\n","Train Epoch: 4 [750/457 (82%)]\tLoss: 0.009419\n","Train Epoch: 4 [800/457 (88%)]\tLoss: 0.010446\n","Train Epoch: 4 [850/457 (93%)]\tLoss: 0.012747\n","Train Epoch: 4 [900/457 (98%)]\tLoss: 0.010819\n","2019-07-22 14:40:03: epoch =    4 , loss = 0.0109 , time = 606.82 s\n","\n","Test set: Average loss: 0.0001\n","\n","Train Epoch: 5 [0/457 (0%)]\tLoss: 0.012324\n","Train Epoch: 5 [50/457 (5%)]\tLoss: 0.011743\n","Train Epoch: 5 [100/457 (11%)]\tLoss: 0.010089\n","Train Epoch: 5 [150/457 (16%)]\tLoss: 0.010681\n","Train Epoch: 5 [200/457 (22%)]\tLoss: 0.008353\n","Train Epoch: 5 [250/457 (27%)]\tLoss: 0.013662\n","Train Epoch: 5 [300/457 (33%)]\tLoss: 0.010363\n","Train Epoch: 5 [350/457 (38%)]\tLoss: 0.008726\n","Train Epoch: 5 [400/457 (44%)]\tLoss: 0.009815\n","Train Epoch: 5 [450/457 (49%)]\tLoss: 0.011637\n","Train Epoch: 5 [500/457 (55%)]\tLoss: 0.010353\n","Train Epoch: 5 [550/457 (60%)]\tLoss: 0.010831\n","Train Epoch: 5 [600/457 (66%)]\tLoss: 0.011362\n","Train Epoch: 5 [650/457 (71%)]\tLoss: 0.010616\n","Train Epoch: 5 [700/457 (77%)]\tLoss: 0.009639\n","Train Epoch: 5 [750/457 (82%)]\tLoss: 0.011406\n","Train Epoch: 5 [800/457 (88%)]\tLoss: 0.010738\n","Train Epoch: 5 [850/457 (93%)]\tLoss: 0.012838\n","Train Epoch: 5 [900/457 (98%)]\tLoss: 0.009801\n","2019-07-22 14:51:03: epoch =    5 , loss = 0.0109 , time = 607.33 s\n","\n","Test set: Average loss: 0.0001\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"EVcD0LnTAP4a","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ez7tS8Yy7_Yi","colab_type":"code","colab":{}},"source":["import os\n","\n","import torch\n","import torch.nn as nn\n","from torchvision import transforms\n","import torchvision\n","\n","import DataSet\n","from Opers import findLastCheckpoint, prepareLoaders\n","\n","save_dir = os.path.join('models', 'ConAutoEncoder')\n","\n","Last_checkpoint = findLastCheckpoint(save_dir)\n","\n","model_location = os.path.join(save_dir, 'model_%03d.pth' % Last_checkpoint)\n","\n","data_transform = transforms.Compose([transforms.ToTensor()])\n","dataset = DataSet.imageDataset('data/normal_images', 'data/normal_images', transform=data_transform)\n","\n","train_loader, validation_loader = prepareLoaders(dataset, shuffle_dataset=True, batch_size=2, )\n","\n","\n","model = torch.load(model_location)\n","\n","device = torch.device('cuda')\n","\n","criterion = nn.MSELoss()\n","\n","samp = next(iter(train_loader))\n","\n","blurry, normal = samp['blurry'].to(device), samp['normal'].to(device)\n","\n","procecced = model(blurry)\n","\n","i=0\n","torchvision.utils.save_image(normal, 'normal_%03d.jpg'%i)\n","\n","torchvision.utils.save_image(procecced, 'procceced_%03d.jpg'%i)\n","\n","torchvision.utils.save_image(blurry, 'blurry_%03d.jpg'%i)\n","  \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_mSvRN9c8fG3","colab_type":"code","colab":{}},"source":["import matplotlib.pyplot as plt\n","def showTensor(aTensor):\n","    plt.figure()\n","    plt.imshow(aTensor.numpy())\n","    plt.colorbar()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z4RqRr_PGrPn","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fT1qCv3MCZv0","colab_type":"code","outputId":"4e3ae376-b9cc-4ff5-8b51-053424ad0c8c","executionInfo":{"status":"error","timestamp":1563723416941,"user_tz":-60,"elapsed":1379,"user":{"displayName":"YOUSSEF ZYAM","photoUrl":"","userId":"03615834181023021007"}},"colab":{"base_uri":"https://localhost:8080/","height":395}},"source":[""],"execution_count":0,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-5111177ce5f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'normal.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocecced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'procceced.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36msave_image\u001b[0;34m(tensor, filename, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     grid = make_grid(tensor, nrow=nrow, padding=padding, pad_value=pad_value,\n\u001b[0;32m--> 101\u001b[0;31m                      normalize=normalize, range=range, scale_each=scale_each)\n\u001b[0m\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Add 0.5 after unnormalizing to [0, 255] to round to nearest integer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mndarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclamp_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/utils.py\u001b[0m in \u001b[0;36mmake_grid\u001b[0;34m(tensor, nrow, padding, normalize, range, scale_each, pad_value)\u001b[0m\n\u001b[1;32m     29\u001b[0m     if not (torch.is_tensor(tensor) or\n\u001b[1;32m     30\u001b[0m             (isinstance(tensor, list) and all(torch.is_tensor(t) for t in tensor))):\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'tensor or list of tensors expected, got {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# if list of tensors, convert to a 4D mini-batch Tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: tensor or list of tensors expected, got <class 'str'>"]}]},{"cell_type":"code","metadata":{"id":"ZKoFsXb4Lb9e","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":238},"outputId":"a7297262-b15d-4156-bb8f-e81ed42961fa","executionInfo":{"status":"error","timestamp":1563808837863,"user_tz":0,"elapsed":777,"user":{"displayName":"YOUSSEF ZYAM","photoUrl":"","userId":"03615834181023021007"}}},"source":["\n","import torchvision \n","\n","for i , data in enumerate(validation_loader):\n","  normal, blurry = data['normal'].to(device), data['blurry'].to(device)\n","\n","  procecced = model(blurry)\n","\n","\n","  torchvision.utils.save_image(normal, 'normal_%03d.jpg'%i)\n","\n","  torchvision.utils.save_image(procecced, 'procceced_%03d.jpg'%i)\n","\n","  torchvision.utils.save_image(blurry, 'blurry_%03d.jpg'%i)\n","  \n","  if i>3: break"],"execution_count":13,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-1fab78b935a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mnormal\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblurry\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'normal'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'blurry'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'validation_loader' is not defined"]}]},{"cell_type":"code","metadata":{"id":"9Dw66WkIRSz2","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}